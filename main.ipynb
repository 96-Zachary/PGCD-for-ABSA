{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预先定义参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logg 生成记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', default='shap_lstm', type=str)\n",
    "parser.add_argument('--dataset', default='restaurant', type=str, help='twitter, restaurant, laptop')\n",
    "parser.add_argument('--optimizer', default='adam', type=str)\n",
    "parser.add_argument('--dropout', default=0.1, type=float)\n",
    "parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
    "parser.add_argument('--learning_rate', default=1e-3, type=float)\n",
    "parser.add_argument('--dropout_rate', default=0.1, type=float)\n",
    "parser.add_argument('--num_epoch', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--embed_dim', default=300, type=int)\n",
    "parser.add_argument('--hidden_dim', default=300, type=int)\n",
    "parser.add_argument('--max_seq_len', default=40, type=int)\n",
    "parser.add_argument('--device', default=None, type=str, help='e.g. cuda:0')\n",
    "parser.add_argument('--seed', default=1234, type=int, help='set seed for reproducibility')\n",
    "parser.add_argument('--l2reg', default=0.001, type=float)\n",
    "parser.add_argument('--valset_ratio', default=0, type=float)\n",
    "parser.add_argument('--polarities_dim', default=3, type=int)\n",
    "\n",
    "\n",
    "parser.add_argument('--bert_dim', default=768, type=int)\n",
    "parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)\n",
    "parser.add_argument('--hops', default=3, type=int)\n",
    "\n",
    "opt = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed for whole model.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "if opt.seed is not None:\n",
    "    print('Set seed for whole model.')\n",
    "    random.seed(opt.seed)\n",
    "    np.random.seed(opt.seed)\n",
    "    torch.manual_seed(opt.seed)\n",
    "    torch.cuda.manual_seed(opt.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LSTM, SHAP_LSTM,ATAE_LSTM, AOA, TNet_LF, SHAP_AOA, SHAP_BERT\n",
    "from models.bert_spc import BERT_SPC\n",
    "\n",
    "model_classes = {\n",
    "    'lstm': LSTM,\n",
    "    'atae_lstm': ATAE_LSTM,\n",
    "    'shap_lstm': SHAP_LSTM,\n",
    "    'aoa': AOA,\n",
    "    'tnet_lf': TNet_LF,\n",
    "    'shap_aoa': SHAP_AOA,\n",
    "    'bert_spc': BERT_SPC,\n",
    "    'shap_bert': SHAP_BERT\n",
    "}\n",
    "\n",
    "# datsets files path\n",
    "dataset_files = {\n",
    "    'twitter': {\n",
    "        'train': './datasets/acl-14-short-data/train.raw',\n",
    "        'test': './datasets/acl-14-short-data/test.raw'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "    }\n",
    "}\n",
    "\n",
    "input_colses = {\n",
    "    'lstm': ['text_raw_indices', 'aspect_indices'],\n",
    "    'td_lstm': ['text_left_with_aspect_indices', 'text_right_with_aspect_indices'],\n",
    "    'tc_lstm': ['text_left_with_aspect_indices', 'text_right_with_aspect_indices', 'aspect_indices'],\n",
    "    'shap_lstm': ['text_raw_indices', 'aspect_indices'],\n",
    "    'atae_lstm': ['text_raw_indices', 'aspect_indices'],\n",
    "    'ian': ['text_raw_indices', 'aspect_indices'],\n",
    "    'memnet': ['text_raw_without_aspect_indices', 'aspect_indices'],\n",
    "    'ram': ['text_raw_indices', 'aspect_indices', 'text_left_indices'],\n",
    "    'cabasc': ['text_raw_indices', 'aspect_indices', 'text_left_with_aspect_indices', 'text_right_with_aspect_indices'],\n",
    "    'tnet_lf': ['text_raw_indices', 'aspect_indices', 'aspect_in_text'],\n",
    "    'aoa': ['text_raw_indices', 'aspect_indices'],\n",
    "    'shap_aoa': ['text_raw_indices', 'aspect_indices'],\n",
    "    'mgan': ['text_raw_indices', 'aspect_indices', 'text_left_indices'],\n",
    "    'bert_spc': ['text_bert_indices', 'bert_segments_ids'],\n",
    "    'aen_bert': ['text_raw_bert_indices', 'aspect_bert_indices'],\n",
    "    'lcf_bert': ['text_bert_indices', 'bert_segments_ids', 'text_raw_bert_indices', 'aspect_bert_indices'],\n",
    "    'shap_bert': ['text_bert_indices', 'bert_segments_ids', 'aspect_bert_indices']\n",
    "}\n",
    "\n",
    "initializers = {\n",
    "    'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "    'xavier_normal_': torch.nn.init.xavier_normal,\n",
    "    'orthogonal_': torch.nn.init.orthogonal_,\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "    'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "    'adam': torch.optim.Adam,  # default lr=0.001\n",
    "    'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "    'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "    'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "    'sgd': torch.optim.SGD,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.model_class = model_classes[opt.model_name]\n",
    "opt.dataset_file = dataset_files[opt.dataset]\n",
    "opt.inputs_cols = input_colses[opt.model_name]\n",
    "opt.initializer = initializers[opt.initializer]\n",
    "opt.optimizer = optimizers[opt.optimizer]\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
    "    if opt.device is None else torch.device(opt.device)\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def pad_and_truncate(sequence, max_seq_len, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = np.zeros((max_seq_len), dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-max_seq_len:]\n",
    "    else:\n",
    "        trunc = sequence[:max_seq_len]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    \n",
    "    return x\n",
    "     \n",
    "# 生成word2idx和idx2word， 以及text2sequence\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, max_seq_len, lower=True):\n",
    "        self.lower = lower\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "    \n",
    "    def fit_on_text(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = list(set(text.split()))\n",
    "        \n",
    "        for idx, word in enumerate(words):\n",
    "            self.word2idx[word] = idx + 1\n",
    "            self.idx2word[idx + 1] = word\n",
    "    \n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        unknownidx = len(self.word2idx) + 1\n",
    "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "class Tokenizer4Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "    \n",
    "def build_tokenizer(fnames, max_seq_len, dat_fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('Pickle data exists! Loading tokenizer.')\n",
    "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        text = ''\n",
    "        for fname in fnames:\n",
    "            find = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "            lines = find.readlines()\n",
    "            find.close()\n",
    "            \n",
    "            for i in range(0, len(lines), 3):\n",
    "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "                text_aspect = lines[i+1].lower().strip()\n",
    "                text_raw = text_left + ' ' + text_aspect + ' ' + text_right\n",
    "                text += text_raw + ' '\n",
    "        tokenizer = Tokenizer(max_seq_len)\n",
    "        tokenizer.fit_on_text(text)\n",
    "        pickle.dump(tokenizer, open('{}'.format(dat_fname), 'wb'))\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
    "    if os.path.exists(dat_fname):\n",
    "        print('Pickle data exists! Loading embedding matrix.')\n",
    "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
    "    else:\n",
    "        print('Loading word vectors ... ')\n",
    "        embedding_matrix = np.zeros((len(word2idx)+2, embed_dim))\n",
    "        fname = './glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
    "            if embed_dim != 300 else './glove.42B.300d.txt'\n",
    "        find = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        word2vec = {}\n",
    "        for lines in find:\n",
    "            tokens = lines.strip().split()\n",
    "            if tokens[0] in word2idx.keys():\n",
    "                word2vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "        \n",
    "        print('Build new embedding matrix.')\n",
    "        for word, idx in word2idx.items():\n",
    "            vec = word2vec.get(word)\n",
    "            if vec is not None:\n",
    "                embedding_matrix[idx, :] = vec\n",
    "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))        \n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle data exists! Loading tokenizer.\n",
      "Pickle data exists! Loading embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "if 'bert' in opt.model_name:\n",
    "    tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.pretrained_bert_name)\n",
    "    bert = BertModel.from_pretrained(opt.pretrained_bert_name)\n",
    "    Model = model_classes[opt.model_name](bert, opt).to(opt.device)\n",
    "    \n",
    "else:\n",
    "    tokenizer = build_tokenizer(\n",
    "        fnames = [opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "        max_seq_len = opt.max_seq_len,\n",
    "        dat_fname = 'params/{0}_tokenizer.dat'.format(opt.dataset)\n",
    "    )\n",
    "    opt.word2idx = tokenizer.word2idx\n",
    "    \n",
    "    embedding_matrix = build_embedding_matrix(tokenizer.word2idx, opt.embed_dim, \n",
    "                                              'params/{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))\n",
    "    model = model_classes[opt.model_name]\n",
    "    Model = model(embedding_matrix, opt).to(opt.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train/test/valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class ABSADatasets(Dataset):\n",
    "    def __init__(self, fname, tokenizer):\n",
    "        find = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        lines = find.readlines()\n",
    "        find.close\n",
    "        \n",
    "        all_data = []\n",
    "        for i in range(0, len(lines), 3):\n",
    "            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition('$T$')]\n",
    "            text_aspect = lines[i+1].lower().strip()\n",
    "            polarity = lines[i+2].strip()\n",
    "            \n",
    "            # General text data\n",
    "            text_raw_indices = tokenizer.text_to_sequence(text_left + ' ' + text_aspect +  ' ' + text_right)\n",
    "            text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + ' ' + text_right)\n",
    "            text_left_indices = tokenizer.text_to_sequence(text_left)\n",
    "            text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + ' ' + text_aspect)\n",
    "            text_right_indices = tokenizer.text_to_sequence(text_right)\n",
    "            text_right_with_aspect_indices = tokenizer.text_to_sequence(text_aspect + ' ' + text_left)\n",
    "            aspect_indices = tokenizer.text_to_sequence(text_aspect)\n",
    "            left_context_len = np.sum(text_left != 0)\n",
    "            aspect_len = np.sum(aspect_indices != 0)\n",
    "            aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])\n",
    "            polarity = int(polarity) + 1\n",
    "            \n",
    "            # Special text BERT data\n",
    "            text_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + text_aspect + \" \" + text_right + ' [SEP] ' + text_aspect + \" [SEP]\")\n",
    "            bert_segments_ids = np.asarray([0] * (np.sum(text_raw_indices != 0) + 2) + [1] * (aspect_len + 1))\n",
    "            bert_segments_ids = pad_and_truncate(bert_segments_ids, tokenizer.max_seq_len)\n",
    "\n",
    "            text_raw_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + text_aspect + \" \" + text_right + \" [SEP]\")\n",
    "            aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_aspect + \" [SEP]\")\n",
    "   \n",
    "            data = {\n",
    "                'text_bert_indices': text_bert_indices[:opt.max_seq_len],\n",
    "                'bert_segments_ids': bert_segments_ids[:opt.max_seq_len],\n",
    "                'text_raw_bert_indices': text_raw_bert_indices[:opt.max_seq_len],\n",
    "                'aspect_bert_indices': aspect_bert_indices[:opt.max_seq_len],\n",
    "                \n",
    "                'text_raw_indices': text_raw_indices[:opt.max_seq_len],\n",
    "                'text_raw_without_aspect_indices': text_raw_without_aspect_indices[:opt.max_seq_len],\n",
    "                'text_left_indices': text_left_indices[:opt.max_seq_len],\n",
    "                'text_left_with_aspect_indices': text_left_with_aspect_indices[:opt.max_seq_len],\n",
    "                'text_right_indices': text_right_indices[:opt.max_seq_len],\n",
    "                'text_right_with_aspect_indices': text_right_with_aspect_indices[:opt.max_seq_len],\n",
    "                'aspect_indices': aspect_indices[:opt.max_seq_len],\n",
    "                'aspect_in_text': aspect_in_text[:opt.max_seq_len],\n",
    "                'polarity': polarity,\n",
    "            }\n",
    "            all_data.append(data)\n",
    "        \n",
    "        self.data = all_data      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3608 1120\n",
      "2887 721 1120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ABSADatasets(dataset_files[opt.dataset]['train'], tokenizer)\n",
    "test_dataset = ABSADatasets(dataset_files[opt.dataset]['test'], tokenizer)\n",
    "\n",
    "print(len(train_dataset.data), len(test_dataset.data))\n",
    "\n",
    "test_dataset = test_dataset.data\n",
    "opt.valset_ratio = 0.2\n",
    "if 1>opt.valset_ratio>0:\n",
    "    valset_len = int(len(train_dataset.data) * opt.valset_ratio)\n",
    "    train_dataset, valid_dataset = random_split(train_dataset.data, (len(train_dataset.data)-valset_len, valset_len))\n",
    "    \n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(dataset=valid_dataset, batch_size=opt.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "max_val_acc, max_val_f1, global_step = 0, 0, 0\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "require_params = filter(lambda p: p.requires_grad, Model.parameters())\n",
    "optimizer = opt.optimizer(require_params, lr=opt.learning_rate, weight_decay=opt.l2reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 1.025 | Batch_Acc: 47.06%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.993 | Batch_Acc: 58.19%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 1.000 | Batch_Acc: 56.38%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.987 | Batch_Acc: 56.74%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.988 | Batch_Acc: 56.62%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.976 | Batch_Acc: 57.41%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.958 | Batch_Acc: 58.85%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.944 | Batch_Acc: 59.54%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.925 | Batch_Acc: 60.60%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.919 | Batch_Acc: 60.40%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.899 | Batch_Acc: 61.29%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.892 | Batch_Acc: 61.51%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.879 | Batch_Acc: 62.11%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.879 | Batch_Acc: 61.80%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.877 | Batch_Acc: 61.50%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.870 | Batch_Acc: 61.81%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.868 | Batch_Acc: 61.74%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.863 | Batch_Acc: 61.89%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.852 | Batch_Acc: 62.60%\n",
      "Epoch:1 | Train_Loss: 0.852 | Train_Acc: 62.60% | Valid_Loss: 0.811 | Valid_Acc: 61.58% | Valid_F1: 0.457 | Epoch_Time: 0m 16.5s |\n",
      "Epoch:2/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.713 | Batch_Acc: 58.82%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.767 | Batch_Acc: 64.41%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.741 | Batch_Acc: 64.99%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.746 | Batch_Acc: 64.39%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.736 | Batch_Acc: 66.82%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.750 | Batch_Acc: 67.93%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.760 | Batch_Acc: 67.04%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.756 | Batch_Acc: 67.72%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.761 | Batch_Acc: 66.92%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.765 | Batch_Acc: 66.58%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.757 | Batch_Acc: 67.22%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.742 | Batch_Acc: 67.98%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.741 | Batch_Acc: 67.94%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.728 | Batch_Acc: 68.57%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.720 | Batch_Acc: 69.25%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.724 | Batch_Acc: 69.30%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.724 | Batch_Acc: 69.03%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.719 | Batch_Acc: 69.49%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.718 | Batch_Acc: 69.39%\n",
      "Epoch:2 | Train_Loss: 0.718 | Train_Acc: 69.39% | Valid_Loss: 0.676 | Valid_Acc: 72.68% | Valid_F1: 0.617 | Epoch_Time: 0m 14.7s |\n",
      "Epoch:3/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.689 | Batch_Acc: 70.59%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.731 | Batch_Acc: 68.36%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.692 | Batch_Acc: 69.73%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.654 | Batch_Acc: 71.83%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.638 | Batch_Acc: 73.06%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.652 | Batch_Acc: 72.34%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.652 | Batch_Acc: 72.36%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.647 | Batch_Acc: 72.47%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.644 | Batch_Acc: 72.78%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.657 | Batch_Acc: 72.61%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.654 | Batch_Acc: 72.48%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.646 | Batch_Acc: 73.04%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.647 | Batch_Acc: 72.95%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.648 | Batch_Acc: 73.15%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.657 | Batch_Acc: 72.80%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.661 | Batch_Acc: 72.49%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.659 | Batch_Acc: 72.60%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.662 | Batch_Acc: 72.42%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.666 | Batch_Acc: 72.26%\n",
      "Epoch:3 | Train_Loss: 0.666 | Train_Acc: 72.26% | Valid_Loss: 0.671 | Valid_Acc: 72.40% | Valid_F1: 0.602 | Epoch_Time: 0m 15.6s |\n",
      "Epoch:4/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.772 | Batch_Acc: 52.94%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.711 | Batch_Acc: 71.75%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.669 | Batch_Acc: 75.07%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.664 | Batch_Acc: 74.04%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.655 | Batch_Acc: 74.73%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.649 | Batch_Acc: 74.30%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.641 | Batch_Acc: 75.23%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.621 | Batch_Acc: 76.25%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.621 | Batch_Acc: 75.87%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.618 | Batch_Acc: 75.84%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.617 | Batch_Acc: 75.94%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.623 | Batch_Acc: 75.80%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.629 | Batch_Acc: 75.43%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.635 | Batch_Acc: 74.96%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.631 | Batch_Acc: 74.79%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.628 | Batch_Acc: 74.72%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.625 | Batch_Acc: 74.85%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.628 | Batch_Acc: 74.79%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.630 | Batch_Acc: 74.62%\n",
      "Epoch:4 | Train_Loss: 0.630 | Train_Acc: 74.62% | Valid_Loss: 0.658 | Valid_Acc: 72.82% | Valid_F1: 0.612 | Epoch_Time: 0m 14.9s |\n",
      "Epoch:5/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.466 | Batch_Acc: 82.35%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.556 | Batch_Acc: 77.40%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.563 | Batch_Acc: 76.85%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.595 | Batch_Acc: 75.45%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.590 | Batch_Acc: 75.95%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.594 | Batch_Acc: 75.76%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.583 | Batch_Acc: 76.36%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.581 | Batch_Acc: 76.17%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.576 | Batch_Acc: 76.56%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.580 | Batch_Acc: 76.32%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.591 | Batch_Acc: 76.00%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.600 | Batch_Acc: 75.86%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.594 | Batch_Acc: 76.05%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.593 | Batch_Acc: 76.11%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.595 | Batch_Acc: 76.03%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.599 | Batch_Acc: 75.92%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.601 | Batch_Acc: 75.90%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.601 | Batch_Acc: 75.70%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.600 | Batch_Acc: 75.66%\n",
      "Epoch:5 | Train_Loss: 0.600 | Train_Acc: 75.66% | Valid_Loss: 0.663 | Valid_Acc: 74.62% | Valid_F1: 0.610 | Epoch_Time: 0m 15.4s |\n",
      "Epoch:6/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.381 | Batch_Acc: 88.24%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.599 | Batch_Acc: 77.97%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.540 | Batch_Acc: 80.12%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.515 | Batch_Acc: 80.28%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.516 | Batch_Acc: 79.45%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.531 | Batch_Acc: 79.31%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.546 | Batch_Acc: 78.40%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.539 | Batch_Acc: 79.16%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.551 | Batch_Acc: 78.26%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.555 | Batch_Acc: 78.24%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.558 | Batch_Acc: 77.98%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.560 | Batch_Acc: 77.43%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.555 | Batch_Acc: 77.80%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.559 | Batch_Acc: 77.49%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.564 | Batch_Acc: 77.40%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.562 | Batch_Acc: 77.53%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.562 | Batch_Acc: 77.38%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.567 | Batch_Acc: 77.09%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.568 | Batch_Acc: 76.90%\n",
      "Epoch:6 | Train_Loss: 0.568 | Train_Acc: 76.90% | Valid_Loss: 0.639 | Valid_Acc: 71.98% | Valid_F1: 0.599 | Epoch_Time: 0m 15.7s |\n",
      "Epoch:7/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.706 | Batch_Acc: 64.71%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.552 | Batch_Acc: 79.10%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.547 | Batch_Acc: 77.15%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Train: 30 | Batch_Avg_Loss: 0.544 | Batch_Acc: 78.07%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.516 | Batch_Acc: 79.30%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.536 | Batch_Acc: 78.46%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.526 | Batch_Acc: 79.22%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.523 | Batch_Acc: 79.33%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.529 | Batch_Acc: 78.95%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.525 | Batch_Acc: 78.86%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.535 | Batch_Acc: 78.35%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.535 | Batch_Acc: 78.50%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.531 | Batch_Acc: 78.88%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.533 | Batch_Acc: 78.83%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.531 | Batch_Acc: 78.91%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.529 | Batch_Acc: 79.02%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.532 | Batch_Acc: 78.89%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.528 | Batch_Acc: 79.06%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.537 | Batch_Acc: 78.64%\n",
      "Epoch:7 | Train_Loss: 0.537 | Train_Acc: 78.64% | Valid_Loss: 0.627 | Valid_Acc: 72.95% | Valid_F1: 0.612 | Epoch_Time: 0m 14.6s |\n",
      "Epoch:8/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.527 | Batch_Acc: 76.47%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.482 | Batch_Acc: 81.36%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.472 | Batch_Acc: 82.79%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.499 | Batch_Acc: 80.48%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.523 | Batch_Acc: 79.15%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.524 | Batch_Acc: 79.07%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.524 | Batch_Acc: 79.22%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.522 | Batch_Acc: 78.80%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.523 | Batch_Acc: 78.87%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.519 | Batch_Acc: 79.20%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.526 | Batch_Acc: 78.91%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.524 | Batch_Acc: 79.07%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.526 | Batch_Acc: 78.83%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.523 | Batch_Acc: 79.07%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.516 | Batch_Acc: 79.35%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.512 | Batch_Acc: 79.48%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.507 | Batch_Acc: 79.67%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.509 | Batch_Acc: 79.58%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.511 | Batch_Acc: 79.43%\n",
      "Epoch:8 | Train_Loss: 0.511 | Train_Acc: 79.43% | Valid_Loss: 0.687 | Valid_Acc: 71.15% | Valid_F1: 0.605 | Epoch_Time: 0m 15.3s |\n",
      "Epoch:9/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.493 | Batch_Acc: 58.82%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.507 | Batch_Acc: 71.75%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.457 | Batch_Acc: 78.93%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.467 | Batch_Acc: 79.48%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.481 | Batch_Acc: 79.45%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.486 | Batch_Acc: 80.54%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.480 | Batch_Acc: 80.86%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.472 | Batch_Acc: 81.27%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.479 | Batch_Acc: 80.88%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.486 | Batch_Acc: 80.65%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.493 | Batch_Acc: 80.09%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.489 | Batch_Acc: 80.53%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.484 | Batch_Acc: 80.69%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.481 | Batch_Acc: 80.88%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.480 | Batch_Acc: 80.99%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.483 | Batch_Acc: 80.89%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.484 | Batch_Acc: 80.79%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.488 | Batch_Acc: 80.45%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.493 | Batch_Acc: 80.23%\n",
      "Epoch:9 | Train_Loss: 0.493 | Train_Acc: 80.23% | Valid_Loss: 0.659 | Valid_Acc: 73.09% | Valid_F1: 0.636 | Epoch_Time: 0m 15.5s |\n",
      "Epoch:10/10; Train Step:******************************\n",
      "Batch Train: 0 | Batch_Avg_Loss: 0.358 | Batch_Acc: 88.24%\n",
      "Batch Train: 10 | Batch_Avg_Loss: 0.486 | Batch_Acc: 79.10%\n",
      "Batch Train: 20 | Batch_Avg_Loss: 0.484 | Batch_Acc: 78.64%\n",
      "Batch Train: 30 | Batch_Avg_Loss: 0.467 | Batch_Acc: 80.28%\n",
      "Batch Train: 40 | Batch_Avg_Loss: 0.452 | Batch_Acc: 81.43%\n",
      "Batch Train: 50 | Batch_Avg_Loss: 0.441 | Batch_Acc: 82.13%\n",
      "Batch Train: 60 | Batch_Avg_Loss: 0.444 | Batch_Acc: 82.60%\n",
      "Batch Train: 70 | Batch_Avg_Loss: 0.447 | Batch_Acc: 82.67%\n",
      "Batch Train: 80 | Batch_Avg_Loss: 0.449 | Batch_Acc: 82.73%\n",
      "Batch Train: 90 | Batch_Avg_Loss: 0.444 | Batch_Acc: 82.84%\n",
      "Batch Train: 100 | Batch_Avg_Loss: 0.446 | Batch_Acc: 82.75%\n",
      "Batch Train: 110 | Batch_Avg_Loss: 0.448 | Batch_Acc: 82.55%\n",
      "Batch Train: 120 | Batch_Avg_Loss: 0.456 | Batch_Acc: 82.14%\n",
      "Batch Train: 130 | Batch_Avg_Loss: 0.460 | Batch_Acc: 82.21%\n",
      "Batch Train: 140 | Batch_Avg_Loss: 0.458 | Batch_Acc: 82.14%\n",
      "Batch Train: 150 | Batch_Avg_Loss: 0.462 | Batch_Acc: 81.92%\n",
      "Batch Train: 160 | Batch_Avg_Loss: 0.462 | Batch_Acc: 81.99%\n",
      "Batch Train: 170 | Batch_Avg_Loss: 0.462 | Batch_Acc: 82.06%\n",
      "Batch Train: 180 | Batch_Avg_Loss: 0.462 | Batch_Acc: 81.86%\n",
      "Epoch:10 | Train_Loss: 0.462 | Train_Acc: 81.86% | Valid_Loss: 0.677 | Valid_Acc: 73.93% | Valid_F1: 0.628 | Epoch_Time: 0m 14.4s |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Train_Loss, Valid_Loss = [], []\n",
    "weights = {}\n",
    "for epoch in range(opt.num_epoch):\n",
    "    epoch_start = time.time()\n",
    "    n_correct, n_total, batch_total_loss = 0, 1, 0.0\n",
    "    print('Epoch:{1}/{0}; Train Step:'.format(opt.num_epoch, epoch+1) + '*'*30)\n",
    "    Model.train()\n",
    "    for num_batch, sample_batch in enumerate(train_data_loader):\n",
    "        global_step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = [sample_batch[col].to(opt.device) for col in opt.inputs_cols]\n",
    "        targets = sample_batch['polarity']\n",
    "        \n",
    "#         # model train code for no shapley\n",
    "#         outputs = Model(inputs)\n",
    "        \n",
    "        if epoch != 4:\n",
    "            outputs = Model(inputs, targets, weights, False)\n",
    "        else:\n",
    "            outputs, weights = Model(inputs, targets, weights, True)\n",
    "        \n",
    "        \n",
    "        batch_loss = loss_func(outputs.cpu(), targets)\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = torch.argmax(outputs, -1).cpu()\n",
    "        n_correct += (preds == targets).sum().item()\n",
    "        n_total += outputs.shape[0]\n",
    "        batch_total_loss += batch_loss.item() * outputs.shape[0]\n",
    "        \n",
    "        if num_batch % 10 == 0:\n",
    "            print('Batch Train: {0} | Batch_Avg_Loss: {1:.3f} | Batch_Acc: {2:.2%}'.format(num_batch, \\\n",
    "                                                                                           batch_total_loss/n_total, \\\n",
    "                                                                                           n_correct/n_total))\n",
    "            \n",
    "    Model.eval()\n",
    "    batch_train_loss = batch_total_loss\n",
    "    batch_train_acc = n_correct/n_total\n",
    "    \n",
    "    pred_label, true_label = [], []\n",
    "    batch_valid_loss, valid_correct, valid_total = 0.0, 0, 0\n",
    "    for num_batch_valid, sample_batch_valid in enumerate(valid_data_loader):\n",
    "        valid_inputs = [sample_batch_valid[col].to(opt.device) for col in opt.inputs_cols]\n",
    "        valid_targets = sample_batch_valid['polarity'].to(opt.device)\n",
    "        # model valid code for no shapley\n",
    "        valid_outputs = Model(valid_inputs, valid_targets, weights, False)\n",
    "#         valid_outputs = Model(valid_inputs)\n",
    "\n",
    "        valid_preds = torch.argmax(valid_outputs, -1)\n",
    "        \n",
    "        batch_valid_loss += (loss_func(valid_outputs, valid_targets)*len(valid_preds)).item()\n",
    "        valid_correct += (valid_preds == valid_targets).sum().item()\n",
    "        valid_total += len(valid_preds)\n",
    "        \n",
    "        pred_label += list(valid_preds.detach().numpy())\n",
    "        true_label+= list(valid_targets.detach().numpy())\n",
    "    \n",
    "    f1 = metrics.f1_score(true_label, pred_label, labels=[0, 1, 2], average='macro')\n",
    "    batch_valid_acc = valid_correct/valid_total\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    mins, secs = epoch_time/60, epoch_time%60\n",
    "    \n",
    "    Train_Loss.append(batch_train_loss/n_total)\n",
    "    Valid_Loss.append(batch_valid_loss/valid_total)\n",
    "    \n",
    "    logger.info('Epoch:{0} | Train_Loss: {1:.3f} | Train_Acc: {2:.2%} | Valid_Loss: {3:.3f} | Valid_Acc: {4:.2%} | Valid_F1: {5:.3f} | Epoch_Time: {6}m {7:.1f}s |'.format(epoch+1, \\\n",
    "                                                                          batch_train_loss/n_total,\\\n",
    "                                                                          batch_train_acc, \\\n",
    "                                                                          batch_valid_loss/valid_total,\\\n",
    "                                                                          batch_valid_acc,\\\n",
    "                                                                          f1, int(mins), secs))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(np.array(Train_Loss), label='Train_Loss')\n",
    "plt.plot(np.array(Valid_Loss), label='Valid_Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentence = 'I love the easy to see screen, and it works well for work.'\n",
    "sentence = sentence.split(' ')\n",
    "\n",
    "while len(sentence) != 15:\n",
    "    sentence.append(' ')\n",
    "# weights[6][:,6] = 0\n",
    "# weights[6][1] = weights[6][0] + weights[6][1]/3\n",
    "df = pd.DataFrame(weights[6][:15].reshape((1,-1)))\n",
    "df.columns = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x57.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,0.8))\n",
    "sns.heatmap(weights[6][:13].reshape((1,-1)), cmap='Reds',linewidths=0.01)\n",
    "plt.xticks(np.arange(len(sentence))+0.5, sentence, fontsize=14, rotation=45)\n",
    "\n",
    "# plt.xlabel('Sentence', Fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = dataset_files[opt.dataset]['train']\n",
    "\n",
    "# find = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "# lines = find.readlines()\n",
    "# find.close\n",
    "\n",
    "# all_data = []\n",
    "# for i in range(0, len(lines), 3):\n",
    "#     text_left, _, text_right = [s.lower().strip() for s in lines[i].partition('$T$')]\n",
    "#     text_aspect = lines[i+1].lower().strip()\n",
    "#     polarity = lines[i+2].strip()\n",
    "#     sentence = text_left + ' ' + text_aspect +' ' + text_right\n",
    "#     sentence = sentence.split()\n",
    "    \n",
    "#     if len(sentence) > 15:     \n",
    "#         if sentence[6] == text_aspect:\n",
    "#             print('Label: ', polarity)\n",
    "#             print('Text: ', text_left + ' *' + text_aspect +'* ' + text_right)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
